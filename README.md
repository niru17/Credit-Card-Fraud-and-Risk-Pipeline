# ğŸ’³ Credit Card Fraud & Risk Detection Pipeline

This project implements a fraud and risk detection data pipeline using **Apache Airflow**, **Apache Spark**, and **BigQuery**, built to automate and scale the processing of transaction and cardholder data.

---

## ğŸ“¦ Project Structure

```
Credit-Card-Fraud-and-Risk-Pipeline/
â”œâ”€â”€ airflow_job.py                 # Airflow DAG to orchestrate the pipeline
â”œâ”€â”€ spark_job.py                   # Spark job to process and classify transactions
â”œâ”€â”€ cardholders.csv                # Cardholder metadata
â”œâ”€â”€ transactions_*.json            # Daily transaction files
â”œâ”€â”€ test_transactions_processing.py # Unit tests for Spark logic
â”œâ”€â”€ requirements.txt               # Python dependencies
â””â”€â”€ README.md                      # Project documentation
```

---

## âš™ï¸ Pipeline Overview

1. **Data Ingestion**  
   Airflow loads JSON transaction files and cardholder data into the pipeline.

2. **Processing with Spark**  
   Spark transforms, cleans, and flags risky transactions using rules or ML models.

3. **Validation & Testing**  
   PySpark transformations are unit-tested using `test_transactions_processing.py`.

4. **Orchestration**  
   Airflow coordinates ingestion, processing, and validation as DAG tasks.

---

## ğŸš€ Technologies Used

- **Apache Airflow** â€“ Task orchestration  
- **Apache Spark (PySpark)** â€“ Distributed data processing  
- **BigQuery (optional)** â€“ For storing curated risk data  
- **GCS / Local Files** â€“ As data sources  
- **Python 3.11** â€“ Runtime  
- **Pytest** â€“ Test framework

---

## ğŸ§ª Input Files

- `cardholders.csv` â€“ Contains cardholder info like name, region, account status  
- `transactions_*.json` â€“ Daily transaction logs with metadata and amount  

---

## âœ… DAG Highlights

| Task                          | Description                          |
|------------------------------|--------------------------------------|
| `start_pipeline`             | Dummy start                          |
| `load_cardholder_data`       | Loads `cardholders.csv`              |
| `load_transaction_data`      | Loads daily JSON files               |
| `process_transactions`       | Applies Spark transformations        |
| `run_tests`                  | Executes PySpark test suite          |
| `end_pipeline`               | Dummy end                            |

---

## ğŸ§  Risk Detection Logic

The Spark job detects high-risk transactions by:
- Filtering suspicious amounts or geolocations
- Flagging blacklisted cardholders
- Enforcing account limits or fraud thresholds

---

## ğŸ“œ Requirements

Install dependencies using:

```bash
pip install -r requirements.txt
```

---

## ğŸ§‘â€ğŸ’» Author

**Niranjana Subramanian**  
_Data Engineer | Data Pipeline Automation Enthusiast_
